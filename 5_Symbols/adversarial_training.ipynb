"""
Complete Adversarial Training Demonstration
Trains two models on MNIST: standard and adversarially trained
Compares their robustness against adversarial attacks
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec

# ============================================================================
# 1. DATA PREPARATION
# ============================================================================

def load_and_preprocess_mnist(num_samples=5000):
    """Load MNIST and return normalized training/test sets"""
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    
    # Normalize to [0, 1]
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0
    
    # Flatten
    x_train = x_train.reshape(-1, 28*28)
    x_test = x_test.reshape(-1, 28*28)
    
    # Limit samples for faster training
    x_train = x_train[:num_samples]
    y_train = y_train[:num_samples]
    
    # One-hot encode
    y_train = tf.keras.utils.to_categorical(y_train, 10)
    y_test = tf.keras.utils.to_categorical(y_test, 10)
    
    return (x_train, y_train), (x_test, y_test)


# ============================================================================
# 2. ADVERSARIAL ATTACK METHODS
# ============================================================================

def fgsm_attack(model, images, labels, epsilon=0.1):
    """
    Fast Gradient Sign Method (FGSM) attack
    One-step attack: perturb in direction of gradient
    """
    images = tf.Variable(images)
    
    with tf.GradientTape() as tape:
        predictions = model(images, training=False)
        loss = tf.keras.losses.categorical_crossentropy(labels, predictions)
    
    gradients = tape.gradient(loss, images)
    signed_grad = tf.sign(gradients)
    adv_images = images + epsilon * signed_grad
    adv_images = tf.clip_by_value(adv_images, 0, 1)
    
    return adv_images.numpy()


def pgd_attack(model, images, labels, epsilon=0.1, alpha=0.01, num_steps=20):
    """
    Projected Gradient Descent (PGD) attack
    Iterative attack: multiple gradient steps with projection
    """
    adv_images = images.copy()
    
    for _ in range(num_steps):
        adv_images = tf.Variable(adv_images)
        
        with tf.GradientTape() as tape:
            predictions = model(adv_images, training=False)
            loss = tf.keras.losses.categorical_crossentropy(labels, predictions)
        
        gradients = tape.gradient(loss, adv_images)
        adv_images = adv_images + alpha * tf.sign(gradients)
        
        # Project back to epsilon ball
        delta = tf.clip_by_value(adv_images - images, -epsilon, epsilon)
        adv_images = (images + delta).numpy()
        adv_images = np.clip(adv_images, 0, 1)
    
    return adv_images


# ============================================================================
# 3. MODEL CREATION
# ============================================================================

def create_simple_model():
    """Create a simple feedforward neural network"""
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    return model


# ============================================================================
# 4. STANDARD TRAINING
# ============================================================================

def train_standard_model(model, x_train, y_train, x_test, y_test, epochs=10):
    """
    Train model on clean data only (standard training)
    """
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print("\n" + "="*70)
    print("STANDARD TRAINING (Clean Data Only)")
    print("="*70)
    
    history = model.fit(
        x_train, y_train,
        batch_size=128,
        epochs=epochs,
        validation_data=(x_test, y_test),
        verbose=1
    )
    
    return history


# ============================================================================
# 5. ADVERSARIAL TRAINING
# ============================================================================

def train_adversarial_model(model, x_train, y_train, x_test, y_test, 
                            epsilon=0.1, epochs=10, attack_type='pgd'):
    """
    Train model on both clean and adversarial examples
    """
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    loss_fn = tf.keras.losses.CategoricalCrossentropy()
    train_metric = tf.keras.metrics.CategoricalAccuracy()
    
    print("\n" + "="*70)
    print(f"ADVERSARIAL TRAINING (Clean + {attack_type.upper()} Examples)")
    print(f"Epsilon: {epsilon}")
    print("="*70)
    
    # Create tf.data pipeline
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    train_dataset = train_dataset.batch(128).shuffle(1000)
    
    history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}
    
    for epoch in range(epochs):
        print(f"\nEpoch {epoch + 1}/{epochs}")
        
        epoch_loss = []
        epoch_acc = []
        
        for step, (batch_images, batch_labels) in enumerate(train_dataset):
            # Generate adversarial examples
            if attack_type == 'pgd':
                adv_examples = pgd_attack(model, batch_images.numpy(), 
                                         batch_labels.numpy(), epsilon=epsilon)
            else:  # FGSM
                adv_examples = fgsm_attack(model, batch_images.numpy(), 
                                          batch_labels.numpy(), epsilon=epsilon)
            
            # Combine clean and adversarial examples
            combined_images = tf.concat([batch_images, adv_examples], axis=0)
            combined_labels = tf.concat([batch_labels, batch_labels], axis=0)
            
            # Training step
            with tf.GradientTape() as tape:
                predictions = model(combined_images, training=True)
                loss = loss_fn(combined_labels, predictions)
            
            gradients = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))
            
            train_metric.update_state(combined_labels, predictions)
            epoch_loss.append(loss.numpy())
            epoch_acc.append(train_metric.result().numpy())
        
        # Validation on clean data
        val_predictions = model(x_test, training=False)
        val_loss = loss_fn(y_test, val_predictions).numpy()
        val_acc = tf.keras.metrics.categorical_accuracy(y_test, val_predictions).numpy().mean()
        
        history['loss'].append(np.mean(epoch_loss))
        history['accuracy'].append(np.mean(epoch_acc))
        history['val_loss'].append(val_loss)
        history['val_accuracy'].append(val_acc)
        
        print(f"Loss: {np.mean(epoch_loss):.4f} | "
              f"Acc: {np.mean(epoch_acc):.4f} | "
              f"Val Loss: {val_loss:.4f} | "
              f"Val Acc: {val_acc:.4f}")
        
        train_metric.reset_state()
    
    return history


# ============================================================================
# 6. EVALUATION & ROBUSTNESS TESTING
# ============================================================================

def evaluate_robustness(model, x_test, y_test, epsilon_values=[0.05, 0.1, 0.15, 0.2]):
    """
    Test model accuracy against adversarial attacks at various epsilon values
    Returns accuracy on clean and adversarial examples
    """
    # Clean accuracy
    clean_predictions = model(x_test, training=False)
    clean_acc = tf.keras.metrics.categorical_accuracy(y_test, clean_predictions).numpy().mean()
    
    results = {'epsilon': [0] + epsilon_values, 'accuracy': [clean_acc]}
    
    print("\nTesting Robustness (PGD Attack)...")
    for epsilon in epsilon_values:
        adv_examples = pgd_attack(model, x_test, y_test.numpy(), epsilon=epsilon, num_steps=10)
        adv_predictions = model(adv_examples, training=False)
        adv_acc = tf.keras.metrics.categorical_accuracy(y_test, adv_predictions).numpy().mean()
        results['accuracy'].append(adv_acc)
        print(f"  Epsilon {epsilon:.2f}: {adv_acc:.4f} accuracy")
    
    return results


# ============================================================================
# 7. VISUALIZATION
# ============================================================================

def visualize_results(standard_history, adversarial_history, 
                     standard_robustness, adversarial_robustness):
    """
    Create comprehensive comparison plots
    """
    fig = plt.figure(figsize=(16, 10))
    gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)
    
    epochs_range = range(1, len(standard_history['loss']) + 1)
    
    # ---- Training Loss ----
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.plot(epochs_range, standard_history['loss'], 'b-o', label='Standard', linewidth=2)
    ax1.plot(epochs_range, adversarial_history['loss'], 'r-s', label='Adversarial', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=11)
    ax1.set_ylabel('Loss', fontsize=11)
    ax1.set_title('Training Loss Comparison', fontsize=12, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ---- Training Accuracy ----
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.plot(epochs_range, standard_history['accuracy'], 'b-o', label='Standard', linewidth=2)
    ax2.plot(epochs_range, adversarial_history['accuracy'], 'r-s', label='Adversarial', linewidth=2)
    ax2.set_xlabel('Epoch', fontsize=11)
    ax2.set_ylabel('Accuracy', fontsize=11)
    ax2.set_title('Training Accuracy Comparison', fontsize=12, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # ---- Validation Accuracy ----
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.plot(epochs_range, standard_history['val_accuracy'], 'b-o', label='Standard', linewidth=2)
    ax3.plot(epochs_range, adversarial_history['val_accuracy'], 'r-s', label='Adversarial', linewidth=2)
    ax3.set_xlabel('Epoch', fontsize=11)
    ax3.set_ylabel('Validation Accuracy', fontsize=11)
    ax3.set_title('Validation Accuracy Comparison', fontsize=12, fontweight='bold')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # ---- Robustness Curve ----
    ax4 = fig.add_subplot(gs[1, :2])
    ax4.plot(standard_robustness['epsilon'], standard_robustness['accuracy'], 
             'b-o', label='Standard Model', linewidth=2.5, markersize=8)
    ax4.plot(adversarial_robustness['epsilon'], adversarial_robustness['accuracy'], 
             'r-s', label='Adversarially Trained Model', linewidth=2.5, markersize=8)
    ax4.set_xlabel('Attack Strength (ε)', fontsize=11)
    ax4.set_ylabel('Accuracy under PGD Attack', fontsize=11)
    ax4.set_title('Robustness Comparison: Standard vs Adversarial Training', 
                  fontsize=12, fontweight='bold')
    ax4.legend(fontsize=11)
    ax4.grid(True, alpha=0.3)
    ax4.set_ylim([0, 1.05])
    
    # ---- Summary Statistics ----
    ax5 = fig.add_subplot(gs[1, 2])
    ax5.axis('off')
    
    summary_text = f"""
SUMMARY STATISTICS

Standard Model:
  • Final Clean Acc: {standard_history['val_accuracy'][-1]:.4f}
  • Robustness @ ε=0.1: {standard_robustness['accuracy'][2]:.4f}
  
Adversarial Model:
  • Final Clean Acc: {adversarial_history['val_accuracy'][-1]:.4f}
  • Robustness @ ε=0.1: {adversarial_robustness['accuracy'][2]:.4f}

Key Insight:
  The adversarial model trades
  some clean accuracy for
  significantly better robustness
  against adversarial attacks.
"""
    
    ax5.text(0.05, 0.95, summary_text, transform=ax5.transAxes,
            fontsize=10, verticalalignment='top', family='monospace',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.suptitle('Adversarial Training Demonstration: Complete Analysis', 
                fontsize=14, fontweight='bold', y=0.995)
    
    return fig


def visualize_adversarial_examples(model_standard, model_adversarial, 
                                   x_test, y_test, num_examples=5):
    """
    Visualize clean vs adversarial vs model predictions
    """
    epsilon = 0.15
    adv_examples = pgd_attack(model_standard, x_test[:num_examples], 
                             y_test[:num_examples].numpy(), epsilon=epsilon)
    
    fig, axes = plt.subplots(num_examples, 4, figsize=(12, 3*num_examples))
    
    for i in range(num_examples):
        clean_img = x_test[i].numpy().reshape(28, 28)
        adv_img = adv_examples[i].reshape(28, 28)
        delta = (adv_img - clean_img)
        
        # Clean image
        ax = axes[i, 0]
        ax.imshow(clean_img, cmap='gray')
        pred_std = model_standard(x_test[i:i+1], training=False).numpy()
        pred_adv = model_adversarial(x_test[i:i+1], training=False).numpy()
        true_label = np.argmax(y_test[i].numpy())
        ax.set_title(f'Clean (True: {true_label})', fontsize=10)
        ax.axis('off')
        
        # Perturbation
        ax = axes[i, 1]
        ax.imshow(delta, cmap='RdBu', vmin=-0.2, vmax=0.2)
        ax.set_title(f'Perturbation (ε={epsilon})', fontsize=10)
        ax.axis('off')
        
        # Adversarial image
        ax = axes[i, 2]
        ax.imshow(adv_img, cmap='gray')
        ax.set_title('Adversarial', fontsize=10)
        ax.axis('off')
        
        # Predictions
        ax = axes[i, 3]
        ax.axis('off')
        pred_std_adv = model_standard(adv_examples[i:i+1], training=False).numpy()
        pred_adv_adv = model_adversarial(adv_examples[i:i+1], training=False).numpy()
        
        text = f"""Standard Model:
  Clean: {np.argmax(pred_std)}
  Adversarial: {np.argmax(pred_std_adv)}
  
Adversarial Model:
  Clean: {np.argmax(pred_adv)}
  Adversarial: {np.argmax(pred_adv_adv)}"""
        
        ax.text(0.1, 0.9, text, transform=ax.transAxes, fontsize=9,
               verticalalignment='top', family='monospace',
               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))
    
    plt.tight_layout()
    return fig


# ============================================================================
# 8. MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*70)
    print("ADVERSARIAL TRAINING COMPLETE DEMONSTRATION")
    print("="*70)
    
    # Load data
    print("\nLoading MNIST data...")
    (x_train, y_train), (x_test, y_test) = load_and_preprocess_mnist(num_samples=5000)
    print(f"Training set: {x_train.shape}, Test set: {x_test.shape}")
    
    # Train standard model
    model_standard = create_simple_model()
    history_standard = train_standard_model(
        model_standard, x_train, y_train, x_test, y_test, epochs=5
    )
    
    # Train adversarial model
    model_adversarial = create_simple_model()
    history_adversarial = train_adversarial_model(
        model_adversarial, x_train, y_train, x_test, y_test, 
        epsilon=0.1, epochs=5, attack_type='pgd'
    )
    
    # Evaluate robustness
    print("\n" + "="*70)
    print("ROBUSTNESS EVALUATION")
    print("="*70)
    
    print("\n[Standard Model]")
    robustness_standard = evaluate_robustness(model_standard, x_test, y_test)
    
    print("\n[Adversarially Trained Model]")
    robustness_adversarial = evaluate_robustness(model_adversarial, x_test, y_test)
    
    # Visualize results
    print("\nGenerating visualizations...")
    fig1 = visualize_results(history_standard, history_adversarial,
                            robustness_standard, robustness_adversarial)
    fig1.savefig('/home/claude/adversarial_comparison.png', dpi=150, bbox_inches='tight')
    print("Saved: adversarial_comparison.png")
    
    fig2 = visualize_adversarial_examples(model_standard, model_adversarial,
                                         x_test, y_test, num_examples=5)
    fig2.savefig('/home/claude/adversarial_examples.png', dpi=150, bbox_inches='tight')
    print("Saved: adversarial_examples.png")
    
    # Print summary
    print("\n" + "="*70)
    print("KEY FINDINGS")
    print("="*70)
    print(f"\nStandard Model:")
    print(f"  Clean Accuracy: {history_standard['val_accuracy'][-1]:.4f}")
    print(f"  Robustness @ ε=0.1: {robustness_standard['accuracy'][2]:.4f}")
    
    print(f"\nAdversarially Trained Model:")
    print(f"  Clean Accuracy: {history_adversarial['val_accuracy'][-1]:.4f}")
    print(f"  Robustness @ ε=0.1: {robustness_adversarial['accuracy'][2]:.4f}")
    
    acc_drop = (history_standard['val_accuracy'][-1] - history_adversarial['val_accuracy'][-1])
    robust_gain = (robustness_adversarial['accuracy'][2] - robustness_standard['accuracy'][2])
    
    print(f"\nTrade-off:")
    print(f"  Clean accuracy drop: {acc_drop:.4f}")
    print(f"  Robustness improvement @ ε=0.1: {robust_gain:.4f}")
    
    print("\n" + "="*70)
    print("Demonstration complete!")
    print("="*70 + "\n")
